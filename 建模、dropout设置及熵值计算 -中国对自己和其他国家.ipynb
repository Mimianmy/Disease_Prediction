{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29546a7-ebb2-4969-adc9-6165c73015af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c0d199f-60a3-41a0-ac18-2f0c8d7ec918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7f98ec-0a26-4ceb-bb21-06a1895e4cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 50% dropout\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)  # 50% dropout\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# 定义模型参数\n",
    "input_size = 10 # 输入特征的维度\n",
    "hidden_size1 = 100 # 第一个隐藏层的大小\n",
    "hidden_size2 = 50 # 第二个隐藏层的大小\n",
    "output_size = 2 # 输出的类别数\n",
    "\n",
    "model = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435f76a1-9dc6-44ad-bedb-cd3c36a73ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe882b04-bb99-4d9b-9a40-11b2754de72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Chinese = pd.read_excel('/Users/anyingbai/Desktop/连续剧/Oxford/COPD模型/最新修改/心血管共病/机器学习模型训练/CHARLS_10_18基线无共病.xlsx', index_col=0)\n",
    "Chinese2=all_Chinese.copy(deep=True)\n",
    "Chinese2.dropna(axis=0, how='any', subset=None, inplace=True)\n",
    "Chinese2.head()\n",
    "Chinese2.isnull().sum()\n",
    "Chinese2_Y = Chinese2['Cardiometabolic_multi_18']\n",
    "Chinese2_X = Chinese2.loc[:,['ADL_Disability','IADL_Disability','edu_group','Male','Married','age','household_wealth','Excessive_drink','physical_activity','smoking_present']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Chinese2_X, Chinese2_Y, test_size = 0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13d05ec-ea6e-4e9f-a8f0-88f7e7204c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fc2724c-a205-4d4e-9647-f3fdab2b0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义训练数据加载器\n",
    "# 可以使用PyTorch的DataLoader来批量加载训练数据\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(torch.Tensor(X_train.values), torch.Tensor(y_train.values))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "for batch_data, batch_labels in train_loader:\n",
    "    batch_data = batch_data.long()\n",
    "    batch_labels = batch_labels.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eba075c-356c-463c-89d3-05cba3f61d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Tensor类型: torch.float32\n",
      "X_train Tensor设备: cpu\n",
      "X_test Tensor类型: torch.float32\n",
      "X_test Tensor设备: cpu\n",
      "y_train Tensor类型: torch.float32\n",
      "y_train Tensor设备: cpu\n",
      "y_test Tensor类型: torch.float32\n",
      "y_test Tensor设备: cpu\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.Tensor(X_train.values)\n",
    "X_test_tensor = torch.Tensor(X_test.values)\n",
    "y_train_tensor = torch.Tensor(y_train.values)\n",
    "y_test_tensor = torch.Tensor(y_test.values)\n",
    "\n",
    "print(\"X_train Tensor类型:\", X_train_tensor.dtype)\n",
    "print(\"X_train Tensor设备:\", X_train_tensor.device)\n",
    "\n",
    "print(\"X_test Tensor类型:\", X_test_tensor.dtype)\n",
    "print(\"X_test Tensor设备:\", X_test_tensor.device)\n",
    "\n",
    "print(\"y_train Tensor类型:\", y_train_tensor.dtype)\n",
    "print(\"y_train Tensor设备:\", y_train_tensor.device)\n",
    "\n",
    "print(\"y_test Tensor类型:\", y_test_tensor.dtype)\n",
    "print(\"y_test Tensor设备:\", y_test_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "931b9b59-36fa-44dc-bba0-777f86ade002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Metrics:\n",
      "Average_Test_Loss: 0.6706273257732391\n",
      "Average_Precision: 0.1469866207429976\n",
      "Average_Recall: 0.1482570281124498\n",
      "Average_F1: 0.14016756087741386\n",
      "Average_AUPRC: 0.16503308565307095\n",
      "Average_AUROC: 0.43256818708833433\n",
      "Average_entropy: 0.9961584454774857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, roc_auc_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 使用训练集的均值和标准差对测试集进行标准化\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# 设置模型结构和参数\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # 在这里定义模型结构，使用给定的最佳参数配置\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(10, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2),\n",
    "            nn.Sigmoid()  # 在输出层使用 Sigmoid 激活函数\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "model = MLP()\n",
    "\n",
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义训练和测试函数\n",
    "def train(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch_data, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, batch_labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return np.mean(train_losses)\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(torch.Tensor(X_test_scaled))\n",
    "        val_loss = criterion(val_outputs, torch.LongTensor(y_test.values))\n",
    "\n",
    "        # 计算预测值\n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        \n",
    "        # 计算熵值\n",
    "        probabilities = F.softmax(val_outputs, dim=1)\n",
    "        entropy = -torch.sum(probabilities * torch.log2(probabilities), dim=1)  # 计算每个样本的熵值\n",
    "        \n",
    "        # 计算平均熵值\n",
    "        average_entropy = torch.mean(entropy).item()\n",
    "        \n",
    "        # 计算评估指标\n",
    "        precision = precision_score(y_test, predicted)\n",
    "        recall = recall_score(y_test, predicted)\n",
    "        f1 = f1_score(y_test, predicted)\n",
    "        \n",
    "        # 对于二分类问题，需要使用模型输出的概率值来计算 AUPRC 和 AUROC\n",
    "        # 获取模型预测的概率值\n",
    "        probabilities = torch.nn.functional.softmax(val_outputs, dim=1)\n",
    "        positive_probabilities = probabilities[:, 1].cpu().numpy()  # 正类的概率\n",
    "        \n",
    "        # 计算 AUPRC 和 AUROC\n",
    "        auprc = average_precision_score(y_test, positive_probabilities)\n",
    "        auroc = roc_auc_score(y_test, positive_probabilities)\n",
    "        \n",
    "        return val_loss.item(), precision, recall, f1, auprc, auroc, average_entropy\n",
    "\n",
    "# 在每个 epoch 中更新测试函数的调用并存储评估指标\n",
    "test_metrics = {'loss': [], 'precision': [], 'recall': [], 'f1': [], 'auprc': [], 'auroc': [], 'entropy': []}\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, optimizer, criterion, train_loader)\n",
    "    test_loss, precision, recall, f1, auprc, auroc, avg_entropy = test(model, X_test_scaled, y_test)  \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # 存储评估指标\n",
    "    test_metrics['loss'].append(test_loss)\n",
    "    test_metrics['precision'].append(precision)\n",
    "    test_metrics['recall'].append(recall)\n",
    "    test_metrics['f1'].append(f1)\n",
    "    test_metrics['auprc'].append(auprc)\n",
    "    test_metrics['auroc'].append(auroc)\n",
    "    test_metrics['entropy'].append(avg_entropy)\n",
    "    \n",
    "# 输出最终评估指标\n",
    "final_metrics = {\n",
    "    'Average_Test_Loss': np.mean(test_metrics['loss']),\n",
    "    'Average_Precision': np.mean(test_metrics['precision']),\n",
    "    'Average_Recall': np.mean(test_metrics['recall']),\n",
    "    'Average_F1': np.mean(test_metrics['f1']),\n",
    "    'Average_AUPRC': np.mean(test_metrics['auprc']),\n",
    "    'Average_AUROC': np.mean(test_metrics['auroc']),\n",
    "    'Average_entropy': np.mean(test_metrics['entropy'])\n",
    "    \n",
    "}\n",
    "print(\"Final Evaluation Metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8507973-9294-4c6f-aeab-25da6bdc7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "#with torch.no_grad():\n",
    "    #test_outputs = model(torch.Tensor(X_test_scaled))\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "# 计算预测概率分布\n",
    "#probabilities = F.softmax(test_outputs, dim=1)\n",
    "#from scipy.stats import entropy\n",
    "\n",
    "# 计算每个数据点的熵\n",
    "#entropies = [entropy(probability, base=2) for probability in probabilities.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e668a948-d9f7-4c81-abfa-164932ab456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整个数据集上每个数据点的平均熵： 0.9956860124112181\n"
     ]
    }
   ],
   "source": [
    "# 假设您已经计算了每个数据点的熵值并存储在了 entropies 列表中\n",
    "# 将列表转换为 PyTorch 张量\n",
    "#entropies_tensor = torch.tensor(entropies)\n",
    "\n",
    "# 计算整个数据集上每个数据点的平均熵\n",
    "#average_entropy = torch.mean(entropies_tensor)\n",
    "\n",
    "# 打印整个数据集上每个数据点的平均熵\n",
    "#print(\"整个数据集上每个数据点的平均熵：\", average_entropy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8f419df-3dcb-4816-b37e-9437d28942a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入欧洲数据\n",
    "all_Europe = pd.read_excel('/Users/anyingbai/Desktop/连续剧/Oxford/COPD模型/最新修改/心血管共病/机器学习模型训练/SHARE_10_18基线无共病.xlsx', index_col=0)\n",
    "Europe2=all_Europe.copy(deep=True)\n",
    "Europe2.dropna(axis=0, how='any', subset=None, inplace=True)\n",
    "Europe2.head()\n",
    "Europe2_Y = Europe2['Cardiometabolic_multi_18']\n",
    "Europe2_X = Europe2.loc[:,['ADL_Disability','IADL_Disability','edu_group','Male','Married','age','household_wealth','Excessive_drink','physical_activity','smoking_present']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Europe2_X, Europe2_Y, test_size = 0.40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccb3b41a-66b8-466d-8a9f-beffd925d702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Metrics:\n",
      "Average_Test_Loss: 0.6341543853282928\n",
      "Average_Precision: 0.1474660404162232\n",
      "Average_Recall: 0.008038922155688626\n",
      "Average_F1: 0.01523538215133514\n",
      "Average_AUPRC: 0.11121311417237635\n",
      "Average_AUROC: 0.41538929699343535\n",
      "Average_entropy: 0.9887345224618912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 使用训均值和标准差对测试集进行标准化\n",
    "scaler = StandardScaler()\n",
    "Europe2_X_scaled = scaler.fit_transform(Europe2_X)\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(torch.Tensor(Europe2_X_scaled))\n",
    "        val_loss = criterion(val_outputs, torch.LongTensor(Europe2_Y.values))\n",
    "\n",
    "        # 计算预测值\n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        \n",
    "        # 计算熵值\n",
    "        probabilities = F.softmax(val_outputs, dim=1)\n",
    "        entropy = -torch.sum(probabilities * torch.log2(probabilities), dim=1)  # 计算每个样本的熵值\n",
    "        \n",
    "        # 计算平均熵值\n",
    "        average_entropy = torch.mean(entropy).item()\n",
    "        \n",
    "        # 计算评估指标\n",
    "        precision = precision_score(Europe2_Y, predicted)\n",
    "        recall = recall_score(Europe2_Y, predicted)\n",
    "        f1 = f1_score(Europe2_Y, predicted)\n",
    "        \n",
    "        # 对于二分类问题，需要使用模型输出的概率值来计算 AUPRC 和 AUROC\n",
    "        # 获取模型预测的概率值\n",
    "        probabilities = torch.nn.functional.softmax(val_outputs, dim=1)\n",
    "        positive_probabilities = probabilities[:, 1].cpu().numpy()  # 正类的概率\n",
    "        \n",
    "        # 计算 AUPRC 和 AUROC\n",
    "        auprc = average_precision_score(Europe2_Y, positive_probabilities)\n",
    "        auroc = roc_auc_score(Europe2_Y, positive_probabilities)\n",
    "        \n",
    "        return val_loss.item(), precision, recall, f1, auprc, auroc, average_entropy\n",
    "\n",
    "# 在每个 epoch 中更新测试函数的调用并存储评估指标\n",
    "test_metrics = {'loss': [], 'precision': [], 'recall': [], 'f1': [], 'auprc': [], 'auroc': [], 'entropy': []}\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, optimizer, criterion, train_loader)\n",
    "    test_loss, precision, recall, f1, auprc, auroc, avg_entropy = test(model, Europe2_X_scaled, Europe2_Y)  \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # 存储评估指标\n",
    "    test_metrics['loss'].append(test_loss)\n",
    "    test_metrics['precision'].append(precision)\n",
    "    test_metrics['recall'].append(recall)\n",
    "    test_metrics['f1'].append(f1)\n",
    "    test_metrics['auprc'].append(auprc)\n",
    "    test_metrics['auroc'].append(auroc)\n",
    "    test_metrics['entropy'].append(avg_entropy)\n",
    "    \n",
    "# 输出最终评估指标\n",
    "final_metrics = {\n",
    "    'Average_Test_Loss': np.mean(test_metrics['loss']),\n",
    "    'Average_Precision': np.mean(test_metrics['precision']),\n",
    "    'Average_Recall': np.mean(test_metrics['recall']),\n",
    "    'Average_F1': np.mean(test_metrics['f1']),\n",
    "    'Average_AUPRC': np.mean(test_metrics['auprc']),\n",
    "    'Average_AUROC': np.mean(test_metrics['auroc']),\n",
    "    'Average_entropy': np.mean(test_metrics['entropy'])\n",
    "    \n",
    "}\n",
    "print(\"Final Evaluation Metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff3e3866-6c5a-493f-a34f-cbc6680bd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入美国数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acabf5a4-fd63-48f2-b939-fcee3ce1f794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Male</th>\n",
       "      <th>edu_group</th>\n",
       "      <th>Married</th>\n",
       "      <th>age</th>\n",
       "      <th>smoking_present</th>\n",
       "      <th>physical_activity</th>\n",
       "      <th>household_wealth</th>\n",
       "      <th>ADL_Disability</th>\n",
       "      <th>IADL_Disability</th>\n",
       "      <th>Excessive_drink</th>\n",
       "      <th>Cardiometabolic_multi_18</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10004040</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004040</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004040</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004040</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004040</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Male  edu_group  Married  age  smoking_present  physical_activity  \\\n",
       "ID                                                                            \n",
       "10004040     0        1.0      1.0   64              0.0                1.0   \n",
       "10004040     0        1.0      1.0   64              0.0                1.0   \n",
       "10004040     0        1.0      1.0   64              0.0                1.0   \n",
       "10004040     0        1.0      1.0   64              0.0                1.0   \n",
       "10004040     0        1.0      1.0   64              0.0                1.0   \n",
       "\n",
       "          household_wealth  ADL_Disability  IADL_Disability  Excessive_drink  \\\n",
       "ID                                                                             \n",
       "10004040         103050000               0                0                1   \n",
       "10004040         103050000               0                0                1   \n",
       "10004040         103050000               0                0                1   \n",
       "10004040         103050000               0                0                1   \n",
       "10004040         103050000               0                0                1   \n",
       "\n",
       "          Cardiometabolic_multi_18  \n",
       "ID                                  \n",
       "10004040                         0  \n",
       "10004040                         0  \n",
       "10004040                         0  \n",
       "10004040                         0  \n",
       "10004040                         0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_US = pd.read_excel('/Users/anyingbai/Desktop/连续剧/Oxford/COPD模型/最新修改/心血管共病/机器学习模型训练/HRS_10_18基线无共病.xlsx', index_col=0)\n",
    "US2=all_US.copy(deep=True)\n",
    "US2.dropna(axis=0, how='any', subset=None, inplace=True)\n",
    "US2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7877be51-0fd2-498a-96fe-f297811f94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "US2_Y = US2['Cardiometabolic_multi_18']\n",
    "US2_X = US2.loc[:,['ADL_Disability','IADL_Disability','edu_group','Male','Married','age','household_wealth','Excessive_drink','physical_activity','smoking_present']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2f6d491-6832-463a-869a-7597ef090225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Metrics:\n",
      "Average_Test_Loss: 0.6370086723566055\n",
      "Average_Precision: 0.1781131411875776\n",
      "Average_Recall: 0.023665968079961307\n",
      "Average_F1: 0.041779166197829225\n",
      "Average_AUPRC: 0.17342077204096737\n",
      "Average_AUROC: 0.4452768954321207\n",
      "Average_entropy: 0.9842941296100617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 使用训均值和标准差对测试集进行标准化\n",
    "scaler = StandardScaler()\n",
    "US2_X_scaled = scaler.fit_transform(US2_X)\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(torch.Tensor(US2_X_scaled))\n",
    "        val_loss = criterion(val_outputs, torch.LongTensor(US2_Y.values))\n",
    "\n",
    "        # 计算预测值\n",
    "        _, predicted = torch.max(val_outputs, 1)\n",
    "        \n",
    "        # 计算熵值\n",
    "        probabilities = F.softmax(val_outputs, dim=1)\n",
    "        entropy = -torch.sum(probabilities * torch.log2(probabilities), dim=1)  # 计算每个样本的熵值\n",
    "        \n",
    "        # 计算平均熵值\n",
    "        average_entropy = torch.mean(entropy).item()\n",
    "        \n",
    "        # 计算评估指标\n",
    "        precision = precision_score(US2_Y, predicted)\n",
    "        recall = recall_score(US2_Y, predicted)\n",
    "        f1 = f1_score(US2_Y, predicted)\n",
    "        \n",
    "        # 对于二分类问题，需要使用模型输出的概率值来计算 AUPRC 和 AUROC\n",
    "        # 获取模型预测的概率值\n",
    "        probabilities = torch.nn.functional.softmax(val_outputs, dim=1)\n",
    "        positive_probabilities = probabilities[:, 1].cpu().numpy()  # 正类的概率\n",
    "        \n",
    "        # 计算 AUPRC 和 AUROC\n",
    "        auprc = average_precision_score(US2_Y, positive_probabilities)\n",
    "        auroc = roc_auc_score(US2_Y, positive_probabilities)\n",
    "        \n",
    "        return val_loss.item(), precision, recall, f1, auprc, auroc, average_entropy\n",
    "\n",
    "# 在每个 epoch 中更新测试函数的调用并存储评估指标\n",
    "test_metrics = {'loss': [], 'precision': [], 'recall': [], 'f1': [], 'auprc': [], 'auroc': [], 'entropy': []}\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, optimizer, criterion, train_loader)\n",
    "    test_loss, precision, recall, f1, auprc, auroc, avg_entropy = test(model, US2_X_scaled, US2_Y)  \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    # 存储评估指标\n",
    "    test_metrics['loss'].append(test_loss)\n",
    "    test_metrics['precision'].append(precision)\n",
    "    test_metrics['recall'].append(recall)\n",
    "    test_metrics['f1'].append(f1)\n",
    "    test_metrics['auprc'].append(auprc)\n",
    "    test_metrics['auroc'].append(auroc)\n",
    "    test_metrics['entropy'].append(avg_entropy)\n",
    "    \n",
    "# 输出最终评估指标\n",
    "final_metrics = {\n",
    "    'Average_Test_Loss': np.mean(test_metrics['loss']),\n",
    "    'Average_Precision': np.mean(test_metrics['precision']),\n",
    "    'Average_Recall': np.mean(test_metrics['recall']),\n",
    "    'Average_F1': np.mean(test_metrics['f1']),\n",
    "    'Average_AUPRC': np.mean(test_metrics['auprc']),\n",
    "    'Average_AUROC': np.mean(test_metrics['auroc']),\n",
    "    'Average_entropy': np.mean(test_metrics['entropy'])\n",
    "    \n",
    "}\n",
    "print(\"Final Evaluation Metrics:\")\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
